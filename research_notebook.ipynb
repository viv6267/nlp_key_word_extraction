{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Text column has contains all the textual data which basically helps us to extract the key words for the text, so ignoring rest columns here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('papers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling laws and local minima in Hebbian ICA\n",
      "\n",
      "Magnus Rattray and Gleb Basalyga\n",
      "Department of Computer Science, University of Manchester,\n",
      "Manchester M13 9PL, UK.\n",
      "magnus@cs.man.ac.uk, basalygg@cs.man.ac.uk\n",
      "\n",
      "Abstract\n",
      "We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we find that a surprisingly\n",
      "large number of examples are required to avoid trapping in a sub-optimal\n",
      "state close to the initial conditions. To extract a skewed signal at least\n",
      "examples are required for -dimensional data and\n",
      "examples are required to extract a symmetrical signal with non-zero kurtosis.\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0006\u0007\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u0002\u0001\t\u0003\u000b\n",
      "\f\u0007\n",
      "\n",
      "1 Introduction\n",
      "Independent component analysis (ICA) is a statistical modelling technique which has attracted a significant amount of research interest in recent years (for a review, see Hyv?arinen,\n",
      "1999). The goal of ICA is to find a representation of data in terms of a combination of statistically independent variables. A number of neural learning algorithms have been applied\n",
      "to this problem, as detailed in the aforementioned review.\n",
      "Theoretical studies of ICA algorithms have mainly focussed on asymptotic stability and\n",
      "efficiency, using the established results of stochastic approximation theory. However, in\n",
      "practice the transient stages of learning will often be more significant in determining the\n",
      "success of an algorithm. In this paper a Hebbian ICA algorithm is analysed in both on-line\n",
      "and batch mode, highlighting the critical importance of the transient dynamics. We find that\n",
      "a surprisingly large number of training examples are required in order to avoid trapping in\n",
      "a sub-optimal state close to the initial conditions. To detect a skewed signal at least\n",
      "examples are required for -dimensional data, while\n",
      "examples are required for a\n",
      "symmetric signal with non-zero kurtosis. In addition, for on-line learning we show that\n",
      "the maximal initial learning rate which allows successful learning is unusually low, being\n",
      "for a skewed signal and\n",
      "for a symmetric signal.\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u0002\u0001\t\u0003\u000f\u000e\u0011\u0010\u0012\u0013\u0007\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u000b\u0005\f\u0007\n",
      "\n",
      "\u0002\u0001\u0004\u0003\n",
      "\f\u0007\n",
      "\n",
      "\u0002\u0001\t\u0003\u0014\u000e\u0015\u0005\f\u0007\n",
      "\n",
      "In order to obtain a tractable model, we consider the limit of high-dimensional data and\n",
      "study an idealised data set in which a single non-Gaussian source is mixed into a large\n",
      "number of Gaussian sources. Recently, one of us considered a more general model in\n",
      "which an arbitrary, but relatively small, number of non-Gaussian sources were mixed into\n",
      "a high-dimensional Gaussian background (Rattray, 2002). In that work a solution to the\n",
      "dynamics of the on-line algorithm was obtained in closed form for\n",
      "learning iterations\n",
      "and a simple solution to the asymptotic dynamics under the optimal learning rate decay was\n",
      "obtained. However, it was noted there that modelling the dynamics on an\n",
      "timescale\n",
      "is not always appropriate, because the algorithm typically requires much longer in order to\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0016\u0007\n",
      "\n",
      "\u0002\u0001\t\u0003\u0016\u0007\n",
      "\n",
      "\fescape from a class of metastable states close to the initial conditions. In order to elucidate\n",
      "this effect in greater detail we focus here on the simplest case of a single non-Gaussian\n",
      "source and we will limit our analysis to the dynamics close to the initial conditions.\n",
      "In recent years a number of on-line learning algorithms, including back-propagation and\n",
      "Sanger?s PCA algorithm, have been studied using techniques from statistical mechanics\n",
      "(see, for example, Biehl (1994); Biehl and Schwarze (1995); Saad and Solla (1995) and\n",
      "contributions in Saad (1998)). These analyses exploited the ?self-averaging? property of\n",
      "certain macroscopic variables in order to obtain ordinary differential equations describing\n",
      "the deterministic evolution of these quantities over time in the large limit. In the present\n",
      "case the appropriate macroscopic quantity does not self-average and fluctuations have to\n",
      "be considered even in the limit. In this case it is more natural to model the on-line learning\n",
      "dynamics as a diffusion process (see, for example Gardiner, 1985).\n",
      "\n",
      "\u0003\n",
      "\n",
      "2 Data Model\n",
      "In order to apply the Hebbian ICA algorithm we must first sphere the data, ie. linearly\n",
      "transform the data so that it has zero mean and an identity covariance matrix. This can be\n",
      "achieved by standard transformations in a batch setting or for on-line learning an adaptive\n",
      "sphering algorithm, such as the one introduced by Cardoso and Laheld (1996), could be\n",
      "used. To simplify the analysis it is assumed here that the data has already been sphered.\n",
      "Without loss of generality it can also be assumed that the sources each have unit variance.\n",
      "\n",
      "\u0001\n",
      "\n",
      "\u0007\u0005\t \u0003\u0019\u0001\u000b\n",
      "\n",
      "\u0018 \f\u000f\u0001\u0004\u000e\u0011\u0003\u001a\u0010 \u000e\u0013\u0002\u001b\u0012 \u0007 \u0007\n",
      "\n",
      "\u0003\u0003\u0002\u0005\u0004\n",
      "\n",
      "Each data point is generated from a noiseless linear mixture of sources which are decomposed into a single non-Gaussian source and\n",
      "uncorrelated Gaussian components,\n",
      ". We will also decompose the mixing matrix into a column vector\n",
      "and a\n",
      "rectangular matrix\n",
      "associated with the non-Gaussian and Gaussian\n",
      "components respectively,\n",
      "\n",
      "\u0014\n",
      "\u0014\u001d\u001c\n",
      "\u001f\n",
      "\u001b\u0014 \u0006\u0003\u0001 !\u001e \u0015 \u0016 \u0001#\"$\u0014%\u001c \u0006'&\n",
      "\n",
      "*\n",
      "\n",
      "\u0015\u0017\u0016\n",
      "\n",
      ")(\n",
      "\n",
      "(1)\n",
      "\n",
      "is presented to the\n",
      "We will consider both the on-line case, in which a new IID example\n",
      "algorithm at each time and then discarded, and also the batch case, in which a finite set\n",
      "of examples are available to the algorithm. To conform with the model assumptions the\n",
      "mixing matrix must be unitary, which leads to the following constraints,\n",
      "\n",
      "\u0014\n",
      "\n",
      "+ \u0015,\u0016-\u0014 \u001c/. \u001f \u001510\u0016 \u001e \u0015,\u00162\u0015 0\u0016 \"3\u0014 \u001c \u0014 0\u001c \u001e 4\u000e \f\n",
      "\u001f \u0015\n",
      "0\u0016 + \u0014%0\u001c\n",
      "\u001f 1\u0015 0\u0016 \u0015 \u0016 \u001510\u0016 \u0014\u001d\u001c %\u001f 7 \n",
      " &\n",
      "\u001450\u001c \u0015 \u0016 \u0014\u001d\u001c . \u001e \u0014%0\u001c \u0015,\u00166\u0014%0\u001c \u0014 \u001c \u001e \n",
      " \u000e\n",
      "\n",
      "(2)\n",
      "(3)\n",
      "\n",
      "3 On-line learning\n",
      "\n",
      "B<:'8%0C\u0015C\u0016 \u0001 8 such\u0007 that the projection 9;:<8=013>@?A\u0001 . Defining\n",
      "9 \u001e 8 0 \u0015 \u0016 \u0001#\"D\u0014\u001d\u001c \u0005 \u0006 \u0002 \u0005\n",
      "(4)\n",
      "\u001e BE\u0001#\"GFIH JKJ 8LJMJ B where F \u0007\u001b\t \u0001\u000bNO\fP\u0004\f\u0007\u0017\f\n",
      "\n",
      "The goal of ICA is to find a vector\n",
      "the overlap\n",
      "we obtain,\n",
      "\n",
      "8\n",
      "RB >S? \u0004 8\n",
      "\n",
      "where we have made use of the constraint in eqn. (2). This assumes zero correlation between and which is true for on-line learning but is only strictly true for the first iteration\n",
      "of batch learning (see section 4). In the algorithm described below we impose a normalisation constraint on such that\n",
      ". In this case we see that the goal is to find such\n",
      "that\n",
      ".\n",
      "\n",
      "JMJ 8LJKJQ\u001e \u0004\n",
      "\n",
      "8\n",
      "\n",
      "\fA simple Hebbian (or anti-Hebbian) learning rule was studied by Hyv?arinen and Oja\n",
      "(1998), who showed it to have a remarkably simple stability condition. We will consider\n",
      "the deflationary form in which a single source is learned at one time. The algorithm is\n",
      "closely related to Projection Pursuit algorithms, which seek interesting projections in highdimensional data. A typical criteria for an interesting projection is to find one which is\n",
      "maximally non-Gaussian in some sense. Maximising some such measure (simple examples would be skewness or kurtosis) leads to the following simple algorithm (see Hyv?arinen\n",
      "and Oja, 1998, for details). The change in at time is given by,\n",
      "\n",
      "8\n",
      "\n",
      "*\n",
      "\n",
      "\u0004 & (5)\n",
      "\u0007 followed\n",
      "by normalisation such that JKJ 8LJKJ\u0011\u001e\n",
      "\u0001 \u0007\n",
      "Here, \u0001 is the learning rate and \u0005 9 is some non-linear function \u0001 which\n",
      "\u0007 we\u0005 will take to be\n",
      "at least three times differentiable. An even non-linearity, eg. \u0005 9 \u001e 9 , is appropriate\n",
      "for\n",
      "detecting asymmetric signals\n",
      "while a more common choice is an odd function, eg.\n",
      "\u0005 \u0001 9 \u0007 \u001e 9 \n",
      " or \u0005 \u0001 9 \u0007 \n",
      "\n",
      "\u001e\n",
      "\t\f\u000b\u000e\n",
      "\u0010\u000f \u0001 9 \u0007\u0002E, \u0004\u0011which\n",
      "used to detect symmetric non-Gaussian\n",
      "\fP\u0015\u0004 \u0014 hascanto bebe chosen\n",
      "signals. In the latter case \u0003\u0012\u0011\u0012\u0013\n",
      "in order to ensure stability of the\n",
      "correct solution, as described by\n",
      "Hyv?\n",
      "a\n",
      "rinen\n",
      "and\n",
      "Oja\n",
      "(1998),\n",
      "either adaptively or using a?\n",
      "\u0004 in the case of an even non-linearity.\n",
      "Remarkably, the same\n",
      "priori knowledge. We set \u0003 \u001e\n",
      "non-linearity can be used to separate both sub and super-Gaussian signals, in contrast to\n",
      "8 \u001e\u0002\u0001\u0004\u0003\u0006\u0005\n",
      "\n",
      "\u00019 (\u0007\n",
      "\n",
      "maximum likelihood methods for which this is typically not the case.\n",
      "We can write the above algorithm as,\n",
      "\n",
      "\u0001 \u0007\n",
      "(6)\n",
      "8 (\u0017\u0016 \u0012 \u001e H \u0004 \"\u001b\u001a\u000e\u0001\u0004\u0003\u00068%\u0005 ( \u0001 9\"\u0018( \u0007 \u0001\u00199 \u0003\u0006( \u0005 \"\u001c9/\u0001 ( \u0005 \u0005 ,(\u0005 \u0001 9 ( \u0007 JMJ ( JMJ \u0005 &\n",
      "\u0003 and \u0001\u001b\u001d \u0002\u0001\u0004\u0003\u000f\u000e\u0013\u0012 \u0007 (two different scalings will be considered below) we can\n",
      "For large\n",
      "expand out to get a weight decay normalisation,\n",
      "8 (\u0017\u0016 \u001f\u0012 \u001e 8 ( \"\u0018\u0001\u0004\u0003\u0006\u0005 \u0001 9 ( \u0007! ( \u0002 9 ( 8 (#\" \u0002 \u0005$\u0012 \u0001 \u0005 \u0003 \u0005 \u0005 \u0001 9 ( \u0007 8 ( &\n",
      "(7)\n",
      "Taking the dot-product with \u0015 \u0016 gives the following update increment for the overlap B ,\n",
      "% B<\u001e&\u0001\u0019\u0003\u0006\u0005 \u0001 9 \u0007 \u0001 ( \u0002 B ( 9 ( \" \u0002 \u0005\u0012 \u0001 \u0005 \u0003 \u0005 \u0005 \u0001 9 ( \u0007 B ( \f\n",
      "(8)\n",
      "where we used %the constraint in eqn. (3) to set \u0015,0\u0016\n",
      "\u001e \u0001 . Below we calculate the mean\n",
      "and variance of B for two different scalings of the learning \u0004rate. Because the conditional\n",
      "in eqn. 4) these expressions\n",
      "distribution for 9 given \u0001 only depends on B (setting JKJ 8LJKJ\u0011\u001e\n",
      "will depend only on B and statistics of the non-Gaussian source distribution.\n",
      "3.1 Dynamics close to the initial conditions\n",
      "\n",
      "\u0015,\u0016 8\n",
      "\n",
      "\u0014 B<\u001e\n",
      "\n",
      "\u0002\u0001\t\u0003 \u000e\u001f\u0012' \u0007\n",
      "\n",
      "If the entries in and are initially of similar order then one would expect\n",
      ".\n",
      "This is the typical case if we consider a random and uncorrelated choice for and the initial\n",
      "entries in . Larger initial values of could only be obtained with some prior knowledge\n",
      "of the mixing matrix which we will not assume. We will set\n",
      "in the following\n",
      "discussion, where is assumed to be an\n",
      "quantity. The discussion below is therefore\n",
      "restricted to describing the dynamics close to the initial conditions. For an account of the\n",
      "transient dynamics far from the initial conditions and the asymptotic dynamics close to an\n",
      "optimal solution, see Rattray (2002).\n",
      "\n",
      "8\n",
      "\n",
      "3.1.1\n",
      "\n",
      "B\n",
      "\n",
      "(\n",
      "\n",
      "(;: B*) \u0003\n",
      "\n",
      "\u0002\u0001 \u0007\n",
      "\n",
      "\u0005 \u0001 9 \u0007 even, + \n",
      "-\u001e , N\n",
      "\n",
      "\u0005 \u00019 \u0007 \u001e 9 \u0005\n",
      "\n",
      "If the signal is asymmetrical then an even non-linearity can be used, for example\n",
      "is a common choice. In this case the appropriate (ie. maximal) scaling for the learning rate\n",
      "is\n",
      "and we set\n",
      "where is an\n",
      "scaled learning rate parameter. In\n",
      "\n",
      "\u0002\u0001\u0004\u0003 \u000e \u0010\u0012 \u0007\n",
      "\n",
      "\u0001 \u001e/.10 \u0003 \u0010\u0012\n",
      "\n",
      ".\n",
      "\n",
      "\u0002\u0001 \u0004\f\u0007\n",
      "\n",
      "\f\u0002\u0001\u0004\u0003\u0006\u0005\n",
      "\u000f \u0001\u0011\u0010\u0012\u0005\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0006\u0005\n",
      "\n",
      "\u0013\n",
      "\n",
      "even, \u0007\u000b\f\u000e\n",
      "\n",
      " \n",
      "\n",
      "%\u0015\u0014\n",
      "\n",
      "\u000f \u0001\u0011\u0010\u0012\u0005\n",
      "\n",
      "\u0010\n",
      "\n",
      "odd, \u0007\u0017\u0016\u0018\f\u000e\n",
      "\n",
      " \n",
      "\n",
      "\u0013 %\u0019\u0014\n",
      "\n",
      "\u0010\n",
      "\n",
      "(\u0005: 8=01\u0015 \u0016 ) \u0003 \u001e \u0002\u0001 \u0007\n",
      "\n",
      "Figure 1: Close to the initial conditions (where\n",
      ") the learning\n",
      "dynamics is equivalent to diffusion in a polynomial potential. For asymmetrical source\n",
      "distributions we can use an even non-linearity in which case the potential is cubic, as shown\n",
      "on the left. For symmetrical source distributions with non-zero kurtosis we should use\n",
      "an odd non-linearity in which case the potential is quartic, as shown on the right. \u0015\n",
      "The\n",
      "\u0014 .\n",
      "with a potential barrier\n",
      "dynamics is initially confined in a metastable state near\n",
      "\n",
      "(E\u001e N\n",
      "\n",
      "%\n",
      "\n",
      "( at each iteration are given\n",
      "+ % \u001e \u0002 \u0005\u0012\u0012 \u001a \u0005 \u0005 \u0001 F \u0007\u001c\u001b . \u0005 ( \" \u0005\u0012 + \n",
      " \u001a \u0005\u0012\u001d \u001d \u0001 F \u0007\u0011\u001b .\u0004( \u0005 \" \u0003 \u000e \u0005 \f\n",
      "E ( .\n",
      "(9)\n",
      "\u001e\n",
      "\u0005\n",
      "\u0005\n",
      "\u000e\n",
      "\u0015\n",
      "\u0005\n",
      "+\n",
      "%\n",
      "\u0001\n",
      "\u001c\n",
      "\u0007\n",
      "\u001b\n",
      "\u0003\n",
      "\f\n",
      "(10)\n",
      "Var ( .\n",
      "\u001a\u0005 F .\n",
      "\u0003 \u000e\u0013\u0012\n",
      "\n",
      "this case we find that the mean and variance of the change in\n",
      "by (to leading order in\n",
      "),\n",
      "\n",
      "+\n",
      "\n",
      "\n",
      "+\u0001% ( \u0007 \u001c . \u001e\n",
      "\n",
      "F \u0007 \t \u0001 ON \f \u0007\n",
      "\n",
      "where\n",
      "is the third cumulant of the source distribution (third central moment), which\n",
      "measures skewness, and brackets denote averages over\n",
      ". We also find that\n",
      "\u000e\u001e\n",
      "E\n",
      "for integer \u001f!\n",
      ". In this case the system can be described by a\n",
      "Fokker-Planck equation for large (see, for example, Gardiner, 1985) with a characteristic\n",
      "timescale of\n",
      ". The system is locally equivalent to a diffusion in the following cubic\n",
      "potential,\n",
      "\u0014\n",
      "\" \u001a \u001c\u001b\n",
      "# \u001a \u001d \u001d \u001c\u001b\n",
      "(11)\n",
      "\n",
      "\t\u0001 \u0003\u000f\u000e \u0007\n",
      "\u0002\u0001\t\u0003 \u0005 \u0007\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u001a\n",
      "\n",
      "\u001e \u0012 \u0005 \u0005 \u0005\u0013\u0001 \u0001 F \u0007 \u0007\u001c\u001b . \u0005 \u0005 ( \u0005 \u0002 \u0012 + \n",
      " \u0005 \u0001 F \u0007 .\u0004( \n",
      " \f\n",
      "with a diffusion coefficient $ %\n",
      "\u001e \u001a \u0005 F . which is independent%\u0015\u0014 ofmust\n",
      "( . The shape of this\n",
      "be overcome to\n",
      "potential is shown on the left of fig. 1. A potential barrier of\n",
      "\u0001( \u0007\n",
      "\n",
      "escape a metastable state close to the initial conditions.\n",
      "3.1.2\n",
      "\n",
      "\u0005 \u0001 9 \u0007 odd, + \" \u001e , N\n",
      "\n",
      "\u0005 \u0001 9 \u0007 \u001e 9 \n",
      " \u0005 \u0001 9 \u0007 \u001e \t \u000b\u000e\n",
      " \u000f \u0001 9 \u0007\n",
      "\u0002\u0001\t\u0003 \u000e\u0015\u0005\f\u0007\n",
      "5\u0001 \u001e . 0 \u0003\u0006\u0005\n",
      "\n",
      "If the signal is symmetrical, or only weakly asymmetrical, it will be necessary to use an\n",
      "odd non-linearity, for example\n",
      "or\n",
      "are popular choices. In\n",
      "this case a lower learning rate is required in order to achieve successful separation. The\n",
      "appropriate scaling for the learning rate is\n",
      "and we set\n",
      "where again is\n",
      "an\n",
      "scaled learning rate parameter. In this case we find that the mean and variance of\n",
      "the change in at each iteration are given by,\n",
      "\n",
      "\u0002\u0001 \u0007\n",
      "\n",
      "(\n",
      "\n",
      ".\n",
      "\n",
      "+ % ( . \u001e \u000f\u0002 \u0005\u0012\u0012 \u001a \u0005 \u0005 \u0001 F \u0007\u001c\u001b . \u0005 ( \" #\u0012 + \" \u001a \u0005 \u001d \u001d \u001d \u0001 F \u0007\u001c\u001b \u0003\u0006.\u0019( \n",
      " \" \u0003 \u000e \n",
      " \f\n",
      "(12)\n",
      "\u001e\n",
      "\u0005\n",
      "\u0005\n",
      "\u0015\n",
      "\u000e\n",
      "\n",
      "\n",
      "+\n",
      "%\n",
      "\u0001\n",
      "\u001c\n",
      "\u0007\n",
      "\u001b\n",
      "\u0003\n",
      "\f\n",
      "\u001a\u0005 F .\n",
      "Var ( .\n",
      "(13)\n",
      "where + \" is the fourth cumulant\n",
      "source distribution (measuring kurtosis) and brackets\n",
      "\u0001\u000bNO\fPof\u0004\f\u0007 .theAgain\n",
      "denote averages over F \u0007\u001b\t\n",
      "the system can be described by a Fokker-Planck\n",
      "E\n",
      "\n",
      "\f\u0002\u0001\u0004\u0003 \n",
      " \u0007\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u0003\n",
      "\n",
      "equation for large\n",
      "but in this case the timescale for learning is\n",
      ", an order of\n",
      "slower than in the asymmetrical case. The system is locally equivalent to diffusion in the\n",
      "following quartic potential,\n",
      "\n",
      "\u001e \"\u0012 \u001a \u0005 \u0005 \u0001 F \u0005 \u0007\u0011\u0001 \u001b . \u0007\u0011\u0005 \u001b ( \u0005 \u0005 \u0002 \u0005 \u0012 \" J + \" \u001a \u0005\u0012\u001d \u001d \u001d \u0001 F \u0007\u001c\u001b J .\u0004( \" \f \u0001 \u0007 (14)\n",
      "with a diffusion coefficient $ \u001e \u001a \u0005 F . . We have assumed \u0003<\u001e Sign + \" which is\n",
      "a necessary condition for successful learning. In the case of a cubic non-linearity this is\n",
      "\u0014 \u0001( \u0007\n",
      "\n",
      "also the condition for stability of the optimal fixed point, although in general these two\n",
      "conditions may not be equivalent (Rattray, 2002). The\n",
      "of this potential is shown\n",
      "\u0015\u0014 shape\n",
      "on the right of fig. 1 and again a potential barrier of\n",
      "must be overcome to escape a\n",
      "metastable state close to the initial conditions.\n",
      "\n",
      "%\n",
      "\n",
      "B<\u001e N\n",
      "For large . the dynamics of ( corresponds to an Ornstein-Uhlenbeck process with a Gaussian stationary distribution of fixed unit variance. Thus, if one chooses too large . initially\n",
      "N (recall, B \u001e (\u00150 ) \u0003 ). As . is reduced\n",
      "the dynamics will become localised close to B \u001e\n",
      "\n",
      "3.1.3 Escape times from a metastable state at\n",
      "\n",
      ".\n",
      "\n",
      "the potential barrier confining the dynamics is reduced. The timescale for escape for large\n",
      "(mean first passage time) is mainly determined by the effective size of the barrier (see,\n",
      "for example, Gardiner, 1985),\n",
      "\n",
      "%\u0015\u0014\n",
      "where\n",
      "\n",
      "\u0001 * \u0007 \u000e\u0013\u0012\n",
      "\u0004\u0003\n",
      "\n",
      "\u0001\n",
      "escape \u0002\n",
      "\n",
      "\u0006\u0007\n",
      "\t\f\u000b\n",
      "\n",
      "%\u0015\u0014\n",
      "\n",
      "$\u000e\n",
      "\n",
      "\f\n",
      "\n",
      "(15)\n",
      "\n",
      "* is a unit of time in\n",
      "\u0001 \u0007\n",
      "N , + .5: \u0001 \u0003 \u0010\u0012 .\n",
      "for even \u0005 9 , + \n",
      " \u001e ,\n",
      "\u0001 \u0007\n",
      "N . + .%:&\u0001 \u0003 \u0005 . (16)\n",
      "for odd \u0005 9 , + \" \u001e ,\n",
      "\n",
      "is the potential barrier, $ is the diffusion coefficient and\n",
      "the diffusion process. For the two cases considered above we obtain,\n",
      "\u0001\n",
      "\n",
      "\u0001\n",
      "\n",
      "even\n",
      "escape\n",
      "\n",
      "\u0002\n",
      "\n",
      "odd\n",
      "escape\n",
      "\n",
      "\u0002\n",
      "\n",
      "\u0004\n",
      "\n",
      "\u0005 \u0001 F \u0007\u0011\u001b . \u0005\u0013\u0012\n",
      "\u001a\n",
      "\u0005\n",
      "\u0004 \u001a + \n",
      " \u001a \u0005 \u001d \u001d \u0001 F \u0007\u0011\u001b \n",
      "\u001f\u0015\u0014 \u001a \u0005 \u0005 \u0001 F \u0007\u0011\u001b .\n",
      "\u0016\n",
      "J + \" \u001a \u0005 \u001d \u001d \u001d \u0001 F \u0007\u0011\u001b J\n",
      "\n",
      "\u0003 \u0005\n",
      "\n",
      "\u0005\u000f\u0007\n",
      "\t\u0011\u0010\n",
      "\n",
      "\u0003 \n",
      "\n",
      "\n",
      "\u0005\u000f\u0007\n",
      "\t\n",
      "\n",
      "\u000b\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u0003\n",
      "\n",
      "The constants of proportionality depend on the shape of the potential and not on . As the\n",
      "learning rate parameter is reduced so the timescale for escape is also reduced. However, the\n",
      "choice of optimal learning rate is non-trivial and cannot be determined by considering only\n",
      "the leading order terms in as above, because although small will result in a quicker\n",
      ", this in turn will lead to a very slow\n",
      "escape from the unstable fixed point near\n",
      "learning transient after escape. Notice that escape time is shortest when the cumulants\n",
      "or \" are large, suggesting that deflationary ICA algorithms will tend to find these signals\n",
      "first.\n",
      "\n",
      "B\n",
      "\n",
      ".\n",
      "\n",
      "B \u001e N\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "\n",
      "\u0002\u0001\u0004\u0003 \u000e\u0013\u0012 \u0007\n",
      "\n",
      "From the above discussion one can draw two important conclusions. Firstly, the initial\n",
      "learning rate must be less than\n",
      "initially in order to avoid trapping close to the\n",
      "initial conditions. Secondly, the number of iterations required to escape the initial transient\n",
      "will be greater than\n",
      ", resulting in an extremely slow initial stage of learning for large\n",
      ". The most extreme case is for symmetric source distributions with non-zero kurtosis, in\n",
      "which case\n",
      "learning iterations are required.\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0016\u0007\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0016\n",
      "\b\u0007\n",
      "\n",
      "In fig. 2 we show results of learning with an asymmetric source (top) and uniform source\n",
      "(bottom) for different scaled learning rates. As the learning rate is increased (left to right)\n",
      "we observe that the dynamics becomes increasingly stochastic, with the potential barrier\n",
      "becoming increasingly significant (potential maxima are shown as dashed lines). For the\n",
      "largest value of learning rate ( \u0018\u0017 ) the algorithm becomes trapped close to the initial\n",
      "conditions for the whole simulation time. From the time axis we observe that the learning\n",
      "timescale is\n",
      "for the asymmetrical signal and\n",
      "for the symmetric signal, as\n",
      "predicted by our theory.\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0006\u0005\f\u0007\n",
      "\n",
      ".$\u001e\n",
      "\n",
      "\u0002\u0001\u0004\u0003\u0016\n",
      "\f\u0007\n",
      "\n",
      "\f?=0.1\n",
      "1\n",
      "\n",
      "?=1\n",
      "\n",
      "?=5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "R\n",
      "\n",
      "R\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "?(x)=x , ? ? 0\n",
      "\n",
      "R\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "t/N2\n",
      "\n",
      "15\n",
      "\n",
      "1\n",
      "R ?(x)=x3, ? ? 0\n",
      "4\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "t/N2\n",
      "\n",
      "15\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "R\n",
      "0.5\n",
      "\n",
      "1\n",
      "R\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "?0.5\n",
      "\n",
      "?0.5\n",
      "\n",
      "?0.5\n",
      "\n",
      "?1\n",
      "0\n",
      "\n",
      "5\n",
      "t/N3\n",
      "\n",
      "+\n",
      " \u001e \u0004&\n",
      "\n",
      "10\n",
      "\n",
      "?1\n",
      "0\n",
      "\n",
      "\u0003\n",
      "\n",
      "\u001e \u0004 NQN\n",
      "\n",
      "5\n",
      "t/N3\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "?1\n",
      "0\n",
      "\n",
      "10\n",
      "t/N2\n",
      "\n",
      "15\n",
      "\n",
      "5\n",
      "t/N3\n",
      "\n",
      "10\n",
      "\n",
      "Figure 2: 100-dimensional data (\n",
      ") is produced from a mixture containing a single\n",
      "non-Gaussian source. In the\u0001 top row we show results for a binary, asymmetrical source with\n",
      "skewness\n",
      ". In the bottom row we show results for a uniformly\n",
      "\u0017 and\n",
      "\u0001\n",
      "distributed source and\n",
      ". Each row shows learning with the same initial conditions\n",
      "and data but with different scaled learning rates (left to right\n",
      "and \u0017 ) where\n",
      "(top) or\n",
      "(bottom). Dashed lines are maxima of the potentials in fig. 1.\n",
      "\n",
      ". : \u0001 \u0003 \u0010\u0012\n",
      "\n",
      ".\n",
      "\n",
      "\u0001 \u0007\n",
      "\u0005\n",
      "\u0001\u0005 \u0005 \u0007 \u001e \u001e \n",
      "\n",
      ":\u0002\u0001 \u0003\u0006\u0005\n",
      "\n",
      ".R\u001e N & \u0004Q\f \u0004\n",
      "\n",
      "4 Batch learning\n",
      "The batch version of eqn. (5) for sufficiently small learning rates can be written,\n",
      "\n",
      "% 8 \u001e \u0019\u0001 \u0003 \u0005 \u0001 9 (  ( \u0002 9 ( 8 ( \"\n",
      "(\u0012\n",
      "\u0002\n",
      "\n",
      "\u0003\n",
      "\n",
      "(17)\n",
      "\n",
      "\u0001\u0004\n",
      "\n",
      "where \u0005 is the number of training examples. Here we argue that such an update requires\n",
      "at least the same order of examples as in the on-line case, in order to be successful. Less\n",
      "data will result in a low signal-to-noise ratio initially and the possibility of trapping in a\n",
      "sub-optimal fixed point close to the initial conditions.\n",
      "As in the on-line case we can write the update in terms of\n",
      "\u0003\n",
      "\n",
      "B\n",
      "\n",
      "B\n",
      "\n",
      ",\n",
      "\n",
      "% B \u001e \u0019\u0001 \u0003 \u0005 \u0001 9 ( \u0007 \u0001 ( \u0002 9 ( B ( \" &\n",
      "(\u0012\n",
      "\u0002\n",
      "\n",
      "N\n",
      "\n",
      "(18)\n",
      "\n",
      "\u0001\u0004\n",
      "\n",
      "We make an assumption that successful learning is unlikely unless the initial increment in\n",
      "is in the desired direction. For example, with an asymmetric signal and quadratic nonlinearity we require\n",
      "initially, while for a symmetric signal and odd non-linearity\n",
      "we require\n",
      "% . We have carried out simulations of batch learning which confirm\n",
      "that a relatively low percentage of runs in which the intial increment was incorrect result\n",
      "in successful learning compared to typical performance. As in the on-line case we observe\n",
      "that runs either succeed, in which case\n",
      ", or fail badly with remaining\n",
      ".\n",
      "\n",
      "%\n",
      "%B B + \n",
      " N B\n",
      "\n",
      "\u0004\n",
      "\u0002\u0001\u0004\u0003 \u000e \u0012' \u0007\n",
      "\b\n",
      "B\n",
      ">\n",
      "?\n",
      "B\n",
      "\u0002\u0001\u0004\u0003\u000f\u000e \u0012\u0013' \u0007 initially and we can therefore expand the right-hand side of\n",
      "As before, B \u001e\n",
      "%\n",
      "\u0003 %\n",
      "eqn. (18) in orders of B for large . B\n",
      "( B at the first iteration) is a sum over raninit\n",
      "\n",
      "\f%B\n",
      "\n",
      "domly sampled terms, and the central limit theorem states that for large \u0005 the distribution\n",
      "init\n",
      "from which\n",
      "is sampled will be Gaussian, with mean and variance given by (to leading order in ),\n",
      "\n",
      "B\n",
      "\n",
      "% B \u0001 \u001e \u0001\u0019\u0003 \u0002 \u0005\u0012 + \n",
      " \u001a \u0005\u0012\u001d \u001d \u0001 F \u0007\u001c\u001b B \u0005 \" #\u0012 + \" \u001a \u0005 \u001d \u001d \u001d \u0001 F \u0007\u0011\u001b B \n",
      " \u0001 \f\n",
      "(19)\n",
      "\u001e\n",
      "\u0005\n",
      "\u0005\n",
      "%\n",
      "\u0001\n",
      "\u001c\n",
      "\u0007\n",
      "\u001b\n",
      "&\n",
      "\u0001\n",
      "Var B\n",
      "(20)\n",
      "\u0001 \u0015\u001a \u0005 F\n",
      "Notice that the + \n",
      " term disappears in the case of an asymmetrical non-linearity, which\n",
      "is why we have left both % terms in eqn. (19). The algorithm will be likely to fail when\n",
      "is of the same order (or greater) than the mean. Since\n",
      "the standard deviation of B\n",
      "\u001e \u0002\u0001\u0004\u0003\u000f\u000e \u0012' \u0007 initially, we see that this is true for R\u001e \u0002\u0001\t\u0003\n",
      "\u0005\f\u0007 in the case of an even nonE\n",
      "\n",
      "init\n",
      "\n",
      "\u0005\n",
      "\n",
      "init\n",
      "\n",
      "\u0005\n",
      "\n",
      "init\n",
      "\n",
      "R\u001e\n",
      "\n",
      "\u0002\u0001\t\u0003\n",
      "\f\u0007\n",
      "\u0005\n",
      "\n",
      "linearity and asymmetric signal, or for \u0005\n",
      "in the case of an odd non-linearity and\n",
      "a signal with non-zero kurtosis. We expect these results to be necessary but not necessarily\n",
      "sufficient for successful learning, since we have only shown that this order of examples is\n",
      "the minimum required to avoid a low signal-to-noise ratio in the first learning iteration. A\n",
      "complete treatment of the batch learning problem would require much more sophisticated\n",
      "formulations such as the mean-field theory of Wong et al. (2000).\n",
      "\n",
      "5 Conclusions and future work\n",
      "In both the batch and on-line Hebbian ICA algorithm we find that a surprisingly large number of examples are required to avoid a sub-optimal fixed point close to the initial conditions. We expect simialr scaling laws to apply in the case of small numbers of non-Gaussian\n",
      "sources. Analysis of the square demixing problem appears to be much more challenging\n",
      "as in this case there may be no simple macroscopic description of the system for large .\n",
      "It is therefore unclear at present whether ICA algorithms based on Maximum-likelihood\n",
      "and Information-theoretic principles (see, for example, Bell and Sejnowski, 1995; Amari\n",
      "et al., 1996; Cardoso and Laheld, 1996), which estimate a square demixing matrix, exhibit\n",
      "similar classes of fixed point to those studied here.\n",
      "\n",
      "\u0003\n",
      "\n",
      "Acknowledgements: This work was supported by an EPSRC award (ref. GR/M48123). We would\n",
      "like to thank Jon Shapiro for useful comments on a preliminary version of this paper.\n",
      "\n",
      "References\n",
      "S-I Amari, A Cichocki, and H H Yang. In D S Touretzky, M C Mozer, and M E Hasselmo, editors, Neural Information Processing Systems 8, pages 757?763. MIT Press,\n",
      "Cambridge MA, 1996.\n",
      "A J Bell and T J Sejnowski. Neural Computation, 7:1129?1159, 1995.\n",
      "M Biehl. Europhys. Lett., 25:391?396, 1994.\n",
      "M Biehl and H Schwarze. J. Phys. A, 28:643?656, 1995.\n",
      "J-F Cardoso and B. Laheld. IEEE Trans. on Signal Processing, 44:3017?3030, 1996.\n",
      "C. W. Gardiner. Handbook of Stochastic Methods. Springer-Verlag, New York, 1985.\n",
      "A Hyv?arinen. Neural Computing Surveys, 2:94?128, 1999.\n",
      "A Hyv?arinen and E Oja. Signal Processing, 64:301?313, 1998.\n",
      "M Rattray. Neural Computation, 14, 2002 (in press).\n",
      "D Saad, editor. On-line Learning in Neural Networks. Cambridge University Press, 1998.\n",
      "D Saad and S A Solla. Phys. Rev. Lett., 74:4337?4340, 1995.\n",
      "K Y M Wong, S Li, and P Luo. In S A Solla, T K Leen, and K-R M?uller, editors, Neural\n",
      "Information Processing Systems 12. MIT Press, Cambridge MA, 2000.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "print(df['paper_text'][1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paper_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to see the duplicate values \n",
    "# ids = df[\"paper_text\"]\n",
    "# df[ids.isin(ids[ids.duplicated()])].sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1578</td>\n",
       "      <td>1998</td>\n",
       "      <td>Dynamics of Supervised Learning with Restricte...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1578-dynamics-of-supervised-learning-with-rest...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Dynamics of Supervised Learning with\\nRestrict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1579</td>\n",
       "      <td>1998</td>\n",
       "      <td>Discovering Hidden Features with Gaussian Proc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1579-discovering-hidden-features-with-gaussian...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Dynamics of Supervised Learning with\\nRestrict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>4497</td>\n",
       "      <td>2012</td>\n",
       "      <td>Emergence of Object-Selective Features in Unsu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4497-emergence-of-object-selective-features-in...</td>\n",
       "      <td>Recent work in unsupervised feature learning h...</td>\n",
       "      <td>Emergence of Object-Selective Features in\\nUns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>4719</td>\n",
       "      <td>2012</td>\n",
       "      <td>Burn-in, bias, and the rationality of anchoring</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4719-burn-in-bias-and-the-rationality-of-ancho...</td>\n",
       "      <td>Bayesian inference provides a unifying framewo...</td>\n",
       "      <td>Emergence of Object-Selective Features in\\nUns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>6052</td>\n",
       "      <td>2016</td>\n",
       "      <td>Only H is left: Near-tight Episodic PAC RL</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6052-only-h-is-left-near-tight-episodic-pac-rl...</td>\n",
       "      <td>In many applications such as advertisement pla...</td>\n",
       "      <td>Launch and Iterate: Reducing Prediction Churn\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5584</th>\n",
       "      <td>6053</td>\n",
       "      <td>2016</td>\n",
       "      <td>Launch and Iterate: Reducing Prediction Churn</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6053-launch-and-iterate-reducing-prediction-ch...</td>\n",
       "      <td>Practical applications of machine learning oft...</td>\n",
       "      <td>Launch and Iterate: Reducing Prediction Churn\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5722</th>\n",
       "      <td>6178</td>\n",
       "      <td>2016</td>\n",
       "      <td>Regret Bounds for Non-decomposable Metrics wit...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6178-regret-bounds-for-non-decomposable-metric...</td>\n",
       "      <td>We consider the problem of recommending releva...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>6879</td>\n",
       "      <td>2017</td>\n",
       "      <td>Mean Field Residual Networks: On the Edge of C...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6879-mean-field-residual-networks-on-the-edge-...</td>\n",
       "      <td>We study randomly initialized residual network...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  year                                              title  \\\n",
       "631   1578  1998  Dynamics of Supervised Learning with Restricte...   \n",
       "632   1579  1998  Discovering Hidden Features with Gaussian Proc...   \n",
       "3862  4497  2012  Emergence of Object-Selective Features in Unsu...   \n",
       "4109  4719  2012    Burn-in, bias, and the rationality of anchoring   \n",
       "5583  6052  2016         Only H is left: Near-tight Episodic PAC RL   \n",
       "5584  6053  2016      Launch and Iterate: Reducing Prediction Churn   \n",
       "5722  6178  2016  Regret Bounds for Non-decomposable Metrics wit...   \n",
       "6499  6879  2017  Mean Field Residual Networks: On the Edge of C...   \n",
       "\n",
       "     event_type                                           pdf_name  \\\n",
       "631         NaN  1578-dynamics-of-supervised-learning-with-rest...   \n",
       "632         NaN  1579-discovering-hidden-features-with-gaussian...   \n",
       "3862        NaN  4497-emergence-of-object-selective-features-in...   \n",
       "4109        NaN  4719-burn-in-bias-and-the-rationality-of-ancho...   \n",
       "5583     Poster  6052-only-h-is-left-near-tight-episodic-pac-rl...   \n",
       "5584     Poster  6053-launch-and-iterate-reducing-prediction-ch...   \n",
       "5722     Poster  6178-regret-bounds-for-non-decomposable-metric...   \n",
       "6499     Poster  6879-mean-field-residual-networks-on-the-edge-...   \n",
       "\n",
       "                                               abstract  \\\n",
       "631                                    Abstract Missing   \n",
       "632                                    Abstract Missing   \n",
       "3862  Recent work in unsupervised feature learning h...   \n",
       "4109  Bayesian inference provides a unifying framewo...   \n",
       "5583  In many applications such as advertisement pla...   \n",
       "5584  Practical applications of machine learning oft...   \n",
       "5722  We consider the problem of recommending releva...   \n",
       "6499  We study randomly initialized residual network...   \n",
       "\n",
       "                                             paper_text  \n",
       "631   Dynamics of Supervised Learning with\\nRestrict...  \n",
       "632   Dynamics of Supervised Learning with\\nRestrict...  \n",
       "3862  Emergence of Object-Selective Features in\\nUns...  \n",
       "4109  Emergence of Object-Selective Features in\\nUns...  \n",
       "5583  Launch and Iterate: Reducing Prediction Churn\\...  \n",
       "5584  Launch and Iterate: Reducing Prediction Churn\\...  \n",
       "5722                                          \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  \n",
       "6499                                          \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(['paper_text'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks duplicate values\n",
    "df['paper_text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicated vals\n",
    "df = df.drop_duplicates(subset='paper_text').reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paper_text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, year, title, event_type, pdf_name, abstract, paper_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(['paper_text'],keep=False)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of stopwords 179\n",
      "{'the', 'here', \"isn't\", 'his', 'whom', 'under', \"wasn't\", 'himself', 'hadn', 'and', 'in', \"won't\", 'to', 'so', \"you'll\", 'them', 'most', 'myself', 'any', 'for', 'what', 'shan', 'until', 'no', 'haven', 'it', 'each', 'isn', 'am', 'will', \"haven't\", 'why', 'your', 'didn', 'those', 'how', 'or', 'out', 'd', 'some', 'mustn', 'into', 'such', 'again', 'these', 'be', 'hasn', 'who', 'yourself', \"hadn't\", 'only', 've', 'my', \"shouldn't\", 'she', 'above', 'themselves', \"that'll\", 'by', 'from', 'once', 'just', \"she's\", 'where', 'too', 'ourselves', 'during', 'yourselves', 'him', 'was', 'they', \"you've\", 'then', 'while', 'very', 'when', 'than', \"hasn't\", 'should', 'won', 'off', 're', \"weren't\", 'hers', 'own', \"you'd\", 'other', 'which', \"couldn't\", 'her', 'more', 'can', 'over', 'we', 'did', \"it's\", \"don't\", 't', 'been', 'mightn', 's', 'because', 'further', 'yours', 'doing', 'about', 'ma', \"should've\", 'down', 'i', 'do', 'against', 'before', \"you're\", 'herself', 'aren', 'you', \"doesn't\", 'itself', 'as', 'is', \"mustn't\", 'not', 'all', \"aren't\", 'he', 'an', 'were', 'ain', 'after', 'of', 'does', 'that', 'our', 'few', 'both', 'having', 'theirs', 'wouldn', 'wasn', 'on', 'y', 'same', 'needn', \"mightn't\", 'between', 'ours', 'if', 'don', 'this', 'nor', 'have', 'll', 'with', 'through', 'below', 'o', 'but', 'there', 'couldn', 'weren', 'doesn', 'are', \"wouldn't\", 'their', 'had', 'me', \"shan't\", 'being', 'a', 'm', 'shouldn', \"didn't\", 'up', 'has', \"needn't\", 'at', 'now', 'its'}\n"
     ]
    }
   ],
   "source": [
    "#gey the reserved stop words\n",
    "from nltk.corpus import stopwords\n",
    "reserved_stop_words = set(stopwords.words('english')) # set >> to get the unique vals\n",
    "print(\"length of stopwords\", len(reserved_stop_words))\n",
    "print(reserved_stop_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of updated stopwords 193\n",
      "{'the', 'here', \"isn't\", 'his', 'whom', 'under', \"wasn't\", 'himself', 'hadn', 'and', 'in', \"won't\", 'to', 'so', \"you'll\", 'them', 'most', 'myself', 'any', 'for', 'what', 'shan', 'until', 'no', 'haven', 'it', 'figure', 'each', 'isn', 'am', 'will', \"haven't\", 'why', 'your', 'didn', 'those', 'how', 'or', 'out', 'fig', 'd', 'some', 'mustn', 'into', 'such', 'again', 'these', 'be', 'hasn', 'sixseven', 'who', 'yourself', \"hadn't\", 'only', 've', 'my', \"shouldn't\", 'she', 'above', 'themselves', \"that'll\", 'by', 'from', 'once', 'just', \"she's\", 'where', 'too', 'ourselves', 'during', 'yourselves', 'him', 'was', 'they', \"you've\", 'then', 'while', 'very', 'when', 'than', \"hasn't\", 'should', 'won', 'off', 're', \"weren't\", 'hers', 'own', \"you'd\", 'other', 'image', 'which', \"couldn't\", 'her', 'more', 'can', 'over', 'we', 'did', \"it's\", \"don't\", 'nine', 't', 'been', 'mightn', 's', 'because', 'two', 'further', 'yours', 'doing', 'about', 'ma', \"should've\", 'down', 'i', 'do', 'against', 'before', \"you're\", 'herself', 'aren', 'you', \"doesn't\", 'itself', 'as', 'is', \"mustn't\", 'not', 'all', \"aren't\", 'he', 'an', 'were', 'ain', 'after', 'of', 'five', 'sample', 'does', 'eight', 'that', 'our', 'few', 'both', 'having', 'theirs', 'wouldn', 'wasn', 'on', 'y', 'same', 'four', 'needn', \"mightn't\", 'between', 'ours', 'if', 'don', 'this', 'nor', 'have', 'll', 'with', 'through', 'below', 'o', 'one', 'but', 'there', 'couldn', 'weren', 'doesn', 'are', \"wouldn't\", 'their', 'had', 'me', \"shan't\", 'being', 'a', 'm', 'ten', 'shouldn', 'using', \"didn't\", 'up', 'three', 'has', \"needn't\", 'at', 'now', 'its'}\n"
     ]
    }
   ],
   "source": [
    "extra_stop_words=['one','two','three','four','five','six' \"seven\",\n",
    "                  \"eight\",\"nine\",'ten','using','sample','fig','figure','image','using']\n",
    "\n",
    "# Add extra stop words to the reserved stop words\n",
    "\n",
    "reserved_stop_words.update(extra_stop_words)\n",
    "print(\"length of updated stopwords\", len(reserved_stop_words))\n",
    "print(reserved_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "1. Lower case\n",
    "2. Remove HTML tags\n",
    "3. Remove special characters and digits\n",
    "4. Convert to list from string (Tokenization)\n",
    "5. Remove stopwords\n",
    "6. Remove words less than three letters\n",
    "7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is testing the html data hello html data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tex=\"This is testing @3347@**8%%$$$$the html data <h1> <p> hello html data </p></h1>\"\n",
    "tex=re.sub(r'<.*?>','',tex).strip()\n",
    "tex=re.sub(r'[^a-zA-Z]+',\" \",tex)\n",
    "\n",
    "\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def processing_text(text):\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub('[^a-zA-Z]+',\" \", text)\n",
    "    \n",
    "    # Convert to list from string\n",
    "    #text = text.split()\n",
    "    text=nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    # Remove words less than three letters\n",
    "    text = [word for word in text if len(word) >= 3]\n",
    "    \n",
    "    # Stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total preprocessing time:  103206.60399999999 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed together application robot eyesight system proposed database associate input output first half part discussion algorithm self organization proposed aspect hardware produce new style neural network latter half part applicability handwritten letter recognition autonomous mobile robot system demonstrated introduction let mapping given finite infinite set another finite infinite set learning machine observes set pair sampled randomly mean cartesian product computes estimate make small estimation error measure usually say faster decrease estimation error increase number sample better learning machine however expression performance incomplete since lack consideration candidate assumed preliminarily find good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating parameter indefinite namely structure equivalent define implicitly set candidate subset mapping computes value parameter based observed sample call type parameter type learning machine defined well approach number sample increase alternative case however estimation error remains eternally thus problem designing learning machine return find proper structure sense hand assumed structure demanded compact possible achieve fast learning word number parameter small since parameter uniquely determined even though observed sample however demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge given though unknown case comparatively easy find proper compact structure alternative case however sometimes difficult possible solution give compactness assume almighty structure cover various combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed sample discussion flourishing since many efficient method proposed recently even hardware unit computing coefficient parallel speed sold anza mark iii odyssey nevertheless neural network always exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose located near case estimation error none negligible however distant estimation error never becomes negligible indeed many research report following situation appears complex estimation error converges value number sample increase decrease hardly even though dimension heighten property sometimes considerable defect neural network recursi type recursive type founded another methodology learning follows initial stage sample set instead notation candidate equal set mapping observing first sample reduced observing second sample reduced thus candidate set becomes gradually small observation sample proceeds observing sample write one likelihood estimation selected hence contrarily parameter type recursive type guarantee surely approach number sample increase recursive type observes sample rewrite value correlated sample hence type architecture composed rule rewriting free memory space architecture form naturally kind database build management system data self organizing way however database differs ordinary one following sense record sample already observed computes estimation call database associative database first subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari dissimilari mean mapping real whenever however necessarily defined single formula definable example collection rule written form dissimilarity defines structure locally hence even though knowledge imperfect flect heuristic way hence contrarily neural network possible accelerate speed learning establishing well especially easily find simple process analogically information like human see application paper recursive type show strongly effectiveness denote sequence observed sample one simplest construction associative database observing sample follows algorithm initial stage let empty set every let equal min furthermore add produce another version improved economize memory follows algorithm initial stage let composed arbitrary element every let lex equal min furthermore let add produce either construction approach increase however computation time grows proportionally size second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sample sequence algorithm constructing associative database follows algorithm step initialization let root root variable assigned respective node memorize data furthermore let step increase put reset pointer root repeat following arrives terminal node leaf notation nand let mean descendant node otherwise let step display yin related information next put yin back step otherwise first establish new descendant node secondly let yin yin yin finally back step loop step stopped time also continued suppose gate element namely artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letter elastic matching method recognizing hand written discrete english letter global training fuzzy logic search method recognizing chinese character written square style etc published self organization associative database realizes recognition handwritten continuous english letter nov fig source document loo fig windowing number sample nualber sampl fig experiment result image scanner take document image fig letter recognizer us parallelogram window least cover maximal letter fig process sequence letter shifting window recognizer scan word slant direction place window left vicinity may first black point detected window catch letter part succeeding letter recognition head letter performed end position namely boundary line two letter becomes known hence starting scanning boundary repeating operation recognizer accomplishes recursively task thus major problem come identifying head letter window considering define following regard window image define accordingly denote black point left area boundary window image project onto window image measure euclidean distance black point closest let summation black point divided number regard couple reading position boundary define accordingly operator teach recognizer interaction relation window image reading boundary algorithm precisely recalled reading incorrect operator teach correct reading via console moreover boundary position incorrect teach correct position via mouse fig show partially document image used experiment fig show change number node recognition rate defined relative frequency correct answer past trial speciiications window height dot width dot slant angular deg example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case however attain since distinguishable excessive lluctuation writing consistency relation assured like number node increase endlessly fig hence clever stop learning recognition rate attains upper limit improve recognition rate must consider spelling word one future subject obstacle avoiding movement various system camera type autonomous mobile robot reported flourishingly system made author fig also belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator therefore motion robot learning becomes natural length width height robot weight visual angle camera deg robot following three factor motion turn le deg advance le control speed le experiment done passageway wid inside building author laboratory exist fig experimental intention arrange box smoking stand gas cylinder stool handcart etc passage way random let robot take image camera recall similar image trace route preliminarily recorded purpose define following let camera face deg downward take image process low pas filter scanning vertically filtered image bottom top search first point luminance change excessively bstitu point bottom white point top black fig obstacle exists front robot white area show free area robot move around regard binary dot image processed thus define accordingly every let number black point exclusive image regard image obtained drawing route image define accordingly robot superimposes current camera image route recalled inquires operator instruction operator judge subjectively whether suggested route appropriate negative answer draw desirable route mouse teach new robot opera tion defines implicitly sample sequence reflecting cost criterion operator iibube roan stationary uni fig configuration autonomous mobile robot system north rmbi unit robot roan fig experimental environment wall camera image preprocessing preprocessing course suggest ion search fig processing obstacle avoiding movement fig processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency fig attains around time notice rest mean directly percentage collision practice prevent collision adopting supplementary measure time number node level tree distributed proposed method reflects delicately various character operator example robot trained operator move slowly enough space obstacle one trained another operator brush quickly obstacle fact give hint method printing character machine position identification robot identify position recalling similar landscape position data camera image purpose principle suffices regard camera image position data respectively however memory capacity finite actual compu ters hence compress camera image slight loss information compression admittable long precision position identification acceptable area thus major problem come find suitable compression method experimental environment fig jut passageway interval section adjacent jut one door robot identifies roughly surrounding landscape section place us temporarily triangular surveying technique exact measure necessary realize former task define following turn camera take panorama image deg scanning horizontally center line substitute point luminance excessively change black point white fig regard binary dot line image processed thus define accordingly every project black point onto measure euclidean distance black point closest let summation similarly calculate exchanging role denoting number respectively nand define regard positive integer labeled section fig define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway within area learns relation landscape position data position identification beyond area achieved crossing plural database one another task automatic excepting periodic reset counter namely kind learning without teacher define identification rate relative frequency correct recall position data past trial typical example converged around time time number level level oftree distributed since identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera image must loosened possibility depends improvement hardware future fig show example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving fig time interval per frame sec fig actual motion robot conclusion method self organizing associative database proposed application robot eyesight system machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced well subclass structure subject imposed widen class probable solution abolish addressing rule depending directly value instead establish another rule depending distribution function value investigation reference hopfield tank computing neural circuit model science rumelhart learning representation back propagating error nature hull hypothesis generation computational model visual word recognition ieee expert fall kurtzberg feature analysis symbol recognition elastic matching ibm re develop wang suen large tree classifier heuristic search global training ieee trans pattern anal mach intell pami brook self calibration motion stereo vision mobile robot int symp robotics research goto stentz cmu system mobile robot navigation ieee int conf robotics automation madarasz design autonomous vehicle disabled ieee jour robotics automation triendl kriegman stereo vision navigation within building ieee int conf robotics automation turk video road following autonomous land vehicle ieee int conf robotics automation'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "corpus=df['paper_text'].apply(lambda x: processing_text(x))\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "execution_time = (end_time - start_time).total_seconds() * 10**3\n",
    "print(\"Total preprocessing time: \", execution_time ,\"ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. unigram would be: ['I','Love','Code']--->[I love to code]\n",
    "2. bigram would be: ['I love','love to','to code']--->[I love, love to, to code]\n",
    "3. trigram would be: ['I love to','love to code']--->[I love to, love to code]\n",
    "\n",
    "max_df=0.95[\"use to ignore 95%  words (occuring mostly in doc)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. smooth_idf=True means this smoothing prevents division by zero when a term is not present is the document.\n",
    "2. use_idf=True means give highest value to less accuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7237\n",
      "['aaai' 'aaron' 'ab' ... 'zij' 'zisserman' 'zoubin']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'comparative'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cv.get_feature_names_out()))\n",
    "\n",
    "#Get feature names\n",
    "feature_name=cv.get_feature_names_out()\n",
    "print(feature_name)\n",
    "feature_name[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cell': 0.4720461002434899, 'cortical': 0.38711519995168037}\n"
     ]
    }
   ],
   "source": [
    "# doc_word_count=tfidf_transformer.transform(cv.transform([corpus[1]]))\n",
    "# #print(doc_word_count.tocoo())\n",
    "# coo=doc_word_count.tocoo()\n",
    "# tupl=zip(coo.col,coo.data)\n",
    "# #print(sorted(tupl,key=lambda x:(x[1],x[0]),reverse=True))\n",
    "# sorted_item=sorted(tupl,key=lambda x:(x[1],x[0]),reverse=True)\n",
    "# sorted_items=sorted_item[:2]\n",
    "# feature_val=[]\n",
    "# score_val=[]\n",
    "# for idx, score in sorted_items:\n",
    "#     feature_val.append(cv.get_feature_names_out()[idx])\n",
    "#     score_val.append(score)\n",
    "\n",
    "# result={}\n",
    "\n",
    "# for i in range(len(feature_val)):\n",
    "#     result[feature_val[i]]=score_val[i]\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============title===============\n",
      "Modelling Spatial Recall, Mental Imagery and Neglect\n",
      "\n",
      "==============abstract===============\n",
      "Abstract Missing\n",
      "\n",
      "==============Keywords===============\n",
      "parietal: 0.533\n",
      "location: 0.278\n",
      "hippocampal: 0.236\n",
      "cell: 0.215\n",
      "object: 0.199\n",
      "place: 0.179\n",
      "texture: 0.173\n",
      "direction: 0.164\n",
      "hippocampus: 0.152\n",
      "representation: 0.151\n",
      "cortex: 0.141\n",
      "spatial: 0.135\n",
      "building: 0.134\n",
      "layer: 0.132\n",
      "activation: 0.124\n",
      "long term: 0.114\n",
      "head: 0.106\n",
      "model: 0.102\n",
      "environment: 0.098\n",
      "recall: 0.093\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "cv=CountVectorizer(max_df=0.95,max_features=len(df),ngram_range=(1,3))\n",
    "word_counts_vector=cv.fit_transform(corpus)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer=tfidf_transformer.fit(word_counts_vector)\n",
    "\n",
    "def get_keywords(idx,corpus,topN):\n",
    "    \n",
    "    doc_word_count=tfidf_transformer.transform(cv.transform([corpus[idx]]))\n",
    "    create_coordinates=doc_word_count.tocoo()\n",
    "    tuple=zip(create_coordinates.col,create_coordinates.data)\n",
    "    sorted_items=sorted(tuple,key=lambda x:(x[1],x[0]),reverse=True)\n",
    "\n",
    "    sorted_items=sorted_items[:topN]\n",
    "\n",
    "    feature_val=[]\n",
    "    score_val=[]\n",
    "    for idx, score in sorted_items:\n",
    "        feature_val.append(cv.get_feature_names_out()[idx])\n",
    "        score_val.append(round(score,3))\n",
    "\n",
    "    result={}\n",
    "\n",
    "    for i in range(len(feature_val)):\n",
    "        result[feature_val[i]]=score_val[i]\n",
    "\n",
    "    return result\n",
    "\n",
    "#keywords=get_keywords(1,corpus,10)\n",
    "\n",
    "\n",
    "\n",
    "def  print_keywords(idx,keywords,df):\n",
    "\n",
    "    print(\"\\n==============title===============\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n==============abstract===============\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n==============Keywords===============\")\n",
    "    for feature_name, score in keywords.items():\n",
    "        print(f\"{feature_name}: {score}\")\n",
    "\n",
    "idx=1001\n",
    "keywords=get_keywords(idx,corpus,topN=20)\n",
    "print(print_keywords(idx,keywords,df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Necessary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "feature_names =cv.get_feature_names_out()\n",
    "pickle.dump(tfidf_transformer,open('Model/tfidf_transformer.pkl','wb'))\n",
    "pickle.dump(cv,open('Model/count_vectorizer.pkl','wb'))\n",
    "pickle.dump(feature_names,open('Model/feature_names.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
